#!/bin/bash

# DcisionAI Platform - Deploy Real AWS Bedrock Inference Profiles
# ==============================================================
#
# This script creates AWS Bedrock Inference Profiles using AWS CLI
# since CloudFormation doesn't support AWS::Bedrock::InferenceProfile

set -e

# Configuration
REGION="us-east-1"
ENVIRONMENT="production"
PLATFORM_NAME="dcisionai"
BASE_MODEL="anthropic.claude-3-sonnet-20240229-v1:0"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check prerequisites
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    # Check AWS CLI
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI is not installed. Please install it first."
        exit 1
    fi
    
    # Check AWS credentials
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS credentials not configured. Please run 'aws configure' first."
        exit 1
    fi
    
    # Check jq for JSON parsing
    if ! command -v jq &> /dev/null; then
        log_error "jq is not installed. Please install it first."
        exit 1
    fi
    
    log_success "Prerequisites check passed"
}

# Check if inference profile exists
profile_exists() {
    local profile_name="$1"
    aws bedrock list-inference-profiles --region "$REGION" --output json | \
        jq -e ".inferenceProfileSummaries[] | select(.profileName == \"$profile_name\")" &> /dev/null
}

# Create inference profile
create_inference_profile() {
    local profile_name="$1"
    local description="$2"
    local optimization_type="$3"
    local regions="$4"
    
    if profile_exists "$profile_name"; then
        log_info "Inference profile $profile_name already exists, skipping..."
        return 0
    fi
    
    log_info "Creating inference profile: $profile_name"
    
    # Create the profile using AWS CLI
    # Note: AWS Bedrock Inference Profiles are created through the console or CLI
    # For now, we'll create a basic profile structure
    
    # This is a simplified approach - in production you'd use the AWS Console
    # or create a more sophisticated CLI-based deployment
    
    log_warning "Inference profile creation requires AWS Console or advanced CLI setup"
    log_info "Profile details:"
    log_info "  Name: $profile_name"
    log_info "  Description: $description"
    log_info "  Model: $BASE_MODEL"
    log_info "  Optimization: $optimization_type"
    log_info "  Regions: $regions"
    
    return 0
}

# Create all inference profiles
create_all_profiles() {
    log_info "Creating all inference profiles..."
    
    # Tier-based profiles
    create_inference_profile \
        "${PLATFORM_NAME}-gold-tier-${ENVIRONMENT}" \
        "Gold tier inference profile for enterprise customers with high performance requirements" \
        "Latency" \
        "us-east-1,us-west-2,eu-west-1,ap-southeast-1"
    
    create_inference_profile \
        "${PLATFORM_NAME}-pro-tier-${ENVIRONMENT}" \
        "Pro tier inference profile for professional customers with balanced performance" \
        "Balanced" \
        "us-east-1,us-west-2,eu-west-1"
    
    create_inference_profile \
        "${PLATFORM_NAME}-free-tier-${ENVIRONMENT}" \
        "Free tier inference profile for basic customers with cost optimization" \
        "Cost" \
        "us-east-1,us-west-2"
    
    # Manufacturing domain profiles
    create_inference_profile \
        "${PLATFORM_NAME}-manufacturing-latency-${ENVIRONMENT}" \
        "Manufacturing domain inference profile optimized for low latency" \
        "Latency" \
        "us-east-1,us-west-2"
    
    create_inference_profile \
        "${PLATFORM_NAME}-manufacturing-cost-${ENVIRONMENT}" \
        "Manufacturing domain inference profile optimized for cost" \
        "Cost" \
        "us-east-1,us-west-2,eu-west-1"
    
    create_inference_profile \
        "${PLATFORM_NAME}-manufacturing-reliability-${ENVIRONMENT}" \
        "Manufacturing domain inference profile optimized for reliability" \
        "Reliability" \
        "us-east-1,us-west-2,eu-west-1,ap-southeast-1"
    
    log_success "All inference profile definitions created"
}

# Create IAM role for Bedrock access
create_iam_role() {
    log_info "Creating IAM role for Bedrock access..."
    
    ROLE_NAME="${PLATFORM_NAME}-bedrock-access-${ENVIRONMENT}"
    
    # Check if role exists
    if aws iam get-role --role-name "$ROLE_NAME" &> /dev/null; then
        log_info "IAM role $ROLE_NAME already exists, skipping..."
        return 0
    fi
    
    # Create trust policy
    cat > /tmp/trust-policy.json << EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "lambda.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
EOF
    
    # Create role
    aws iam create-role \
        --role-name "$ROLE_NAME" \
        --assume-role-policy-document file:///tmp/trust-policy.json \
        --description "IAM role for DcisionAI Platform to access Bedrock Inference Profiles"
    
    # Attach managed policy
    aws iam attach-role-policy \
        --role-name "$ROLE_NAME" \
        --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
    
    # Create custom policy
    cat > /tmp/bedrock-policy.json << EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel",
                "bedrock:InvokeModelWithResponseStream",
                "bedrock:ListInferenceProfiles",
                "bedrock:GetInferenceProfile"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "bedrock-runtime:InvokeModel",
                "bedrock-runtime:InvokeModelWithResponseStream"
            ],
            "Resource": "*"
        }
    ]
}
EOF
    
    # Create and attach custom policy
    POLICY_NAME="${PLATFORM_NAME}-bedrock-policy-${ENVIRONMENT}"
    aws iam create-policy \
        --policy-name "$POLICY_NAME" \
        --policy-document file:///tmp/bedrock-policy.json
    
    POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)
    aws iam attach-role-policy --role-name "$ROLE_NAME" --policy-arn "$POLICY_ARN"
    
    # Cleanup temp files
    rm -f /tmp/trust-policy.json /tmp/bedrock-policy.json
    
    log_success "IAM role $ROLE_NAME created successfully"
}

# List existing inference profiles
list_profiles() {
    log_info "Listing existing inference profiles..."
    
    PROFILES=$(aws bedrock list-inference-profiles --region "$REGION" --output json)
    
    # Check for our profiles
    PROFILE_NAMES=(
        "${PLATFORM_NAME}-gold-tier-${ENVIRONMENT}"
        "${PLATFORM_NAME}-pro-tier-${ENVIRONMENT}"
        "${PLATFORM_NAME}-free-tier-${ENVIRONMENT}"
        "${PLATFORM_NAME}-manufacturing-latency-${ENVIRONMENT}"
        "${PLATFORM_NAME}-manufacturing-cost-${ENVIRONMENT}"
        "${PLATFORM_NAME}-manufacturing-reliability-${ENVIRONMENT}"
    )
    
    log_info "Checking for DcisionAI inference profiles..."
    for profile_name in "${PROFILE_NAMES[@]}"; do
        if echo "$PROFILES" | jq -e ".inferenceProfileSummaries[] | select(.profileName == \"$profile_name\")" &> /dev/null; then
            log_success "âœ“ Inference profile $profile_name exists"
        else
            log_warning "âš  Inference profile $profile_name not found"
        fi
    done
}

# Show manual creation instructions
show_manual_instructions() {
    log_info "Manual Inference Profile Creation Instructions:"
    echo
    echo "Since AWS Bedrock Inference Profiles cannot be created via CloudFormation,"
    echo "you need to create them manually in the AWS Console:"
    echo
    echo "1. Go to AWS Bedrock Console: https://console.aws.amazon.com/bedrock/"
    echo "2. Navigate to 'Inference profiles'"
    echo "3. Click 'Create inference profile'"
    echo "4. Use the following configurations:"
    echo
    echo "Profile Names to Create:"
    echo "  - ${PLATFORM_NAME}-gold-tier-${ENVIRONMENT}"
    echo "  - ${PLATFORM_NAME}-pro-tier-${ENVIRONMENT}"
    echo "  - ${PLATFORM_NAME}-free-tier-${ENVIRONMENT}"
    echo "  - ${PLATFORM_NAME}-manufacturing-latency-${ENVIRONMENT}"
    echo "  - ${PLATFORM_NAME}-manufacturing-cost-${ENVIRONMENT}"
    echo "  - ${PLATFORM_NAME}-manufacturing-reliability-${ENVIRONMENT}"
    echo
    echo "Base Model: $BASE_MODEL"
    echo "Regions: us-east-1, us-west-2, eu-west-1, ap-southeast-1 (as appropriate)"
    echo
    echo "After creating profiles, update the inference manager configuration"
    echo "with the actual profile ARNs."
}

# Main execution
main() {
    log_info "ðŸš€ Starting DcisionAI Real Inference Profile Deployment"
    log_info "Region: $REGION"
    log_info "Environment: $ENVIRONMENT"
    log_info "Platform: $PLATFORM_NAME"
    log_info "Base Model: $BASE_MODEL"
    
    echo
    
    # Execute deployment steps
    check_prerequisites
    create_all_profiles
    create_iam_role
    list_profiles
    show_manual_instructions
    
    echo
    log_success "ðŸŽ‰ Inference Profile Deployment Setup Completed!"
    log_info "Next steps:"
    log_info "1. Create inference profiles manually in AWS Bedrock Console"
    log_info "2. Update inference manager with real profile ARNs"
    log_info "3. Test the MCP server integration"
    log_info "4. Run the complete architecture tests"
}

# Handle script interruption
trap 'log_error "Deployment interrupted"; exit 1' INT TERM

# Run main function
main "$@"
